{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Class Preprocessing\n",
    "This notebook will perform preprocessing on all but one class to test the model's ability to identify new classes. The preprocessing steps taken are:\n",
    "\n",
    "1. Min/max scaling\n",
    "2. PCA\n",
    "\n",
    "There are a total of 10 possible classes. Benign traffic will be present in all subsets, and each type of possible attack will be left out once, resulting in 9 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"Benign\": 0,\n",
    "    \"ACK_Flooding\": 1,\n",
    "    \"ARP_Spoofing\": 2,\n",
    "    \"Port_Scanning\": 3,\n",
    "    \"Service_Detection\": 4,\n",
    "    \"SYN_Flooding\": 5,\n",
    "    \"UDP_Flooding\": 6,\n",
    "    \"HTTP_Flooding\": 7,\n",
    "    \"Telnet-brute_Force\": 8,\n",
    "    \"Host_Discovery\": 9,\n",
    "}\n",
    "\n",
    "# Set below unknown_attacks to be the types of traffic that will be tested as novel\n",
    "unknown_attacks = [\"ACK_Flooding\", \"ARP_Spoofing\", \"Port_Scanning\", \"Service_Detection\", \"SYN_Flooding\", \"UDP_Flooding\", \"HTTP_Flooding\", \"Telnet-brute_Force\", \"Host_Discovery\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw data\n",
    "train_set = pd.read_csv(\"./sampled_datasets/train_set.csv\")\n",
    "val_set = pd.read_csv(\"./sampled_datasets/val_set.csv\")\n",
    "test_set = pd.read_csv(\"./sampled_datasets/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:611: RuntimeWarning: invalid value encountered in matmul\n",
      "  C = X.T @ X\n"
     ]
    }
   ],
   "source": [
    "# Scale features\n",
    "columns_to_normalize = [\"length\"] + [f\"kit_fe_{i}\" for i in range(0, 100)]\n",
    "\n",
    "for attack in unknown_attacks:\n",
    "    # Filter out the unknown attack label from training set\n",
    "    new_train = train_set[train_set[\"label\"] != labels[attack]].reset_index(drop=True)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(new_train[columns_to_normalize])\n",
    "\n",
    "    # Scale training, validation, and test sets\n",
    "    train_norm = pd.DataFrame(scaler.transform(new_train[columns_to_normalize]), columns=columns_to_normalize)\n",
    "    val_norm = pd.DataFrame(scaler.transform(val_set[columns_to_normalize]), columns=columns_to_normalize)\n",
    "    test_norm = pd.DataFrame(scaler.transform(test_set[columns_to_normalize]), columns=columns_to_normalize)\n",
    "\n",
    "    # Concatenate normalized columns back to the non-normalized columns for the training data\n",
    "    train_norm = pd.concat([new_train.drop(columns=columns_to_normalize), train_norm], axis=1)\n",
    "    val_norm = pd.concat([val_set.drop(columns=columns_to_normalize), val_norm], axis=1)\n",
    "    test_norm = pd.concat([test_set.drop(columns=columns_to_normalize), test_norm], axis=1)\n",
    "\n",
    "    # Initialize PCA and fit to the scaled training data\n",
    "    pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
    "    pca.fit(train_norm.drop('label', axis=1))\n",
    "\n",
    "    # Save PCA-transformed train data\n",
    "    X_train_pca =  pca.transform(train_norm.drop('label', axis=1))\n",
    "    train_pca_df = pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(X_train_pca.shape[1])])\n",
    "    train_pca_df['label'] = train_norm['label'].values\n",
    "    train_pca_df.to_csv(f\"./new_class_datasets/pca_train_no_{attack}.csv\", index=False)\n",
    "\n",
    "    # Apply PCA to the test and validation sets\n",
    "    X_test_pca = pca.transform(test_norm.drop('label', axis=1))\n",
    "    test_pca_df = pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(X_test_pca.shape[1])])\n",
    "    test_pca_df['label'] = test_norm['label'].values\n",
    "    test_pca_df.to_csv(f\"./new_class_datasets/pca_test_no_{attack}.csv\", index=False)\n",
    "\n",
    "    X_val_pca = pca.transform(val_norm.drop('label', axis=1))\n",
    "    val_pca_df = pd.DataFrame(X_val_pca, columns=[f'PC{i+1}' for i in range(X_val_pca.shape[1])])\n",
    "    val_pca_df['label'] = val_norm['label'].values\n",
    "    val_pca_df.to_csv(f\"./new_class_datasets/pca_val_no_{attack}.csv\", index=False)\n",
    "\n",
    "    # Save min-max scaler and PCA model\n",
    "    joblib.dump(scaler, f'./preprocessing_models/scaler_no_{attack}.joblib')\n",
    "    joblib.dump(pca, f'./preprocessing_models/pca_no_{attack}.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we leave one class out to simulate an unknown attack, map the labels so that they are sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential relabelling\n",
    "current_labels = list(labels.values())\n",
    "current_labels.remove(labels[attack])\n",
    "\n",
    "# Create a mapping from original labels to new labels\n",
    "label_mapping = {label: i for i, label in enumerate(current_labels)}\n",
    "\n",
    "# Add label for unknown class\n",
    "label_mapping[labels[attack]] = -1\n",
    "\n",
    "# Apply the mapping to dataset\n",
    "train_pca_df['label'] = train_pca_df['label'].map(label_mapping)\n",
    "val_pca_df['label'] = val_pca_df['label'].map(label_mapping)\n",
    "test_pca_df['label'] = test_pca_df['label'].map(label_mapping)\n",
    "\n",
    "train_pca_df.to_csv(f\"./new_class_datasets/pca_train_no_{attack}.csv\", index=False)\n",
    "val_pca_df.to_csv(f\"./new_class_datasets/pca_val_no_{attack}.csv\", index=False)\n",
    "test_pca_df.to_csv(f\"./new_class_datasets/pca_test_no_{attack}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
